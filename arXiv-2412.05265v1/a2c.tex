
\begin{algorithm}
\dontprintsemicolon
\caption{Advantage actor critic (A2C)}
Initialize actor parameters $\vtheta$,
critic parameters $\vw$ \\
\For{$e=1:E$}
       {
         Obtain starting observation $\vo_1$ of a new episode \\
         Init RNN state $\vh_1=\text{init}(\vo_1)$ \\
       \For{$t=1:T$}
           {
            // Step \\
       $\va_t \sim \pi_{\vtheta}(\cdot|\vh_t)$ \\
       $(\vo_{t+1}, r_{t+1}) = \text{env.step}(\va_t)$ \\

       // State update \\
       $\vh_{t+1} = \text{dyn}(\vh_{t}, \vo_{t+1})$ \\

       // Loss signal \\
       $\delta_t = r_{t+1} + \gamma \stopgrad(V_{\vw}(\vh_{t+1})) - V_{\vw}(\vh_t)$ \\
       
       // Parameter update \\
            $\vw  \assign \vw + \lr_{\vw} \delta_t \nabla_{\vw} V_{\vw}(\vh_t)$ \\
        $\vtheta \assign \vtheta +
        \lr_{\vtheta} \gamma^t \delta_t \nabla_{\vtheta} \log \pi_{\vtheta}(\va_t|\vh_t)$ \\

           }
}
\end{algorithm}



\begin{algorithm}
\dontprintsemicolon
\caption{Advantage actor critic (A2C) with partial world model}
Initialize actor parameters $\vtheta$,
critic parameters $\vw$,
model parameters $\valpha$\\
\For{$e=1:E$}
       {
         Obtain starting observation $\vo_1$ of a new episode \\
             $\vh_1 = \text{init}(\vo_1)$ \\
       \For{$t=1:T$}
           {
            // Step \\
      $\va_t \sim \pi_{\vtheta}(\cdot|\vh_t)$ \\
      $(\vo_{t+1}, r_{t+1}) = \text{env.step}(\va_t)$ \\

            // State update \\
       $\vh_{t+1} = \text{dyn}(\vh_{t}, \vo_{t+1}, \vtheta)$ \\
            
        // Error signals \\
        $\delta_t = r_{t+1} + \gamma \stopgrad(V_{\vw}(\vh_{t+1})) - V_{\vw}(\vh_t)$ \\
%        $L_t = \text{loss}(\text{features}(\vo_{t+1}),
        %        \text{pred}(\vh_t, \va_t, \valpha))$ \\
        $\hat{\vD}_{t+1}=        \text{pred}(\vh_t, \va_t, \valpha)$ \\
        $\vD_{t+1} = \text{features}(\vo_{t+1})$ \\
        $L_{t+1} = \text{loss}(\vD_{t+1}, \hat{\vD}_{t+1})$ \\
       
        // Parameter update \\
         $\vw  \assign \vw + \lr_{\vw} \delta \nabla_{\vw} V_{\vw}(\vh_t)$ \\
        $\vtheta \assign \vtheta +
        \lr_{\vtheta} \gamma^t \delta_t \nabla_{\vtheta} \log \pi_{\vtheta}(\va_t|\vh_t)$ \\
        $\valpha \assign \valpha + \lr_{\valpha}  \nabla_{\valpha} L_t$ \\
       }
}
\end{algorithm}


\begin{algorithm}
\dontprintsemicolon
\caption{Advantage actor critic (A2C) with shared partial world model}
Initialize actor parameters $\vtheta$,
critic parameters $\vw$,
predictor parameters $\valpha$,
RNN parameters $\vpsi$\\
\For{$e=1:E$}
       {
         Obtain starting observation $\vo_1$ of a new episode \\
             $\vh_1 = \text{init}(\vo_1)$ \\
       \For{$t=1:T$}
           {
            // Step \\
      $\va_t \sim \pi_{\vtheta}(\cdot|\vh_t)$ \\
      $(\vo_{t+1}, r_{t+1}) = \text{env.step}(\va_t)$ \\
            $\vD_{t+1} = \text{features}(\vo_{t+1})$ \\
       $\vh_{t+1} = \text{dyn}(\vh_{t}, \vo_{t+1}, \vpsi)$ \\
            
            // Error signals \\
            $\delta_t(\vw,\vpsi) = r_{t+1} +
            \gamma \stopgrad(V_{\vw}(\vh_{t+1})) - V_{\vw}(\vh_t)$ // critic\\
            $\loss_t^{\pi}(\vtheta,\vpsi) = \delta_t \gamma^t
              \log \pi_{\vtheta}(\va_t|\vh_t)$ // actor \\
              $\loss_{t}^{D}(\valpha,\vpsi) = \text{loss}(\vD_{t+1},
              \text{pred}(\vh_t, \va_t, \valpha))$ // predictor\\
              $\loss_t(\vw,\vtheta,\valpha,\vpsi) =
              \lambda_1 \delta_t(\vw,\vpsi)
              + \lambda_2 \loss_t^{\pi}(\vtheta,\vpsi)
              + \lambda_3 \loss_{t}^{D}(\valpha,\vpsi)$ // combined \\
       
        // Parameter update \\
        $\vw  \assign \vw + \lr_{\vw} \nabla_{\vw} \loss_t$\\
        $\vtheta  \assign \vtheta + \lr_{\vtheta} \nabla_{\vtheta} \loss_t$\\
        $\valpha  \assign \vpsi + \lr_{\valpha} \nabla_{\valpha} \loss_t$\\
        $\vpsi  \assign \vpsi + \lr_{\vpsi} \nabla_{\vpsi} \loss_t$
       }
}
\end{algorithm}



\begin{algorithm}
\dontprintsemicolon
\caption{Advantage actor critic (A2C) with partial world model OLD}
Initialize actor parameters $\vtheta$,
critic parameters $\vw$,
model parameters $\valpha$\\
\For{$e=1:E$}
       {
         Obtain starting observation $\vo_1$ of a new episode \\
                     $\vp_1 = f(\vo_1)$  \\
             $\vz_{1} = \text{enc}(\vp_t, \valpha)$ \\
             $\vh_1 = \text{init}(\vo_1, \vz_1)$ \\
       \For{$t=1:T$}
           {
            // Step \\
      $\va_t \sim \pi_{\vtheta}(\cdot|\vh_t)$ \\
      $(\vo_{t+1}, r_{t+1}) = \text{env.step}(\va_t)$ \\

            // State update \\
            $\vp_{t+1} = f(\vo_{t+1})$ \\
       $\vz_{t+1} = \text{enc}(\vp_{t+1}, \valpha)$ \\
       $\vh_{t+1} = \text{dyn}(\vh_{t}, \vo_{t+1}, \vz_{t+1})$ \\
            $\hat{\vp}_{t+1} = \text{pred}(\vz_t, \valpha)$ \\
            
        // Error signals \\
        $\delta_t = r_{t+1} + \gamma \stopgrad(V_{\vw}(\vh_{t+1})) - V_{\vw}(\vh_t)$ \\
        $L_t = \text{loss}(\vp_{t+1}, \hat{\vp}_{t+1})$ \\
       
        // Parameter update \\
         $\vw  \assign \vw + \lr_{\vw} \delta \nabla_{\vw} V_{\vw}(\vh_t)$ \\
        $\vtheta \assign \vtheta +
        \lr_{\vtheta} \gamma^t \delta_t \nabla_{\vtheta} \log \pi_{\vtheta}(\va_t|\vh_t)$ \\
        $\valpha \assign \valpha + \lr_{\valpha}  \nabla_{\valpha} L_t$ \\
       }
}
\end{algorithm}

